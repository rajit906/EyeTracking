{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCyZzQ92j9iV"
      },
      "source": [
        "M2L School 2025 Edition, Milan, Italy.\n",
        "\n",
        "This tutorial reuses material from:\n",
        "*   Explanations and code snippets from the [2023 edition](https://github.com/M2Lschool/tutorials2024/).\n",
        "\n",
        "Notebook Author: Matteo Hessel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn0W-ok9j9iV"
      },
      "source": [
        "---\n",
        "# RL02: Deep RL with Policy Gradient methods.\n",
        "---\n",
        "\n",
        "Welcome to the second part of the RL Lab @ M2L 2025!\n",
        "\n",
        "This notebook introduces you to a fundamental concept in reinforcement learning: **Policy Gradient Methods**. Why should we care? Many real-world problems involve continuous action spaces or complex environments where value-based methods struggle to succeed. Policy gradient methods offer an alternative approach by directly parametrising and learning the policy, making them powerful tools for these kinds of challenges.\n",
        "\n",
        "Our goal in this notebook is to take you step-by-step through the process of building and training an agent using policy gradient methods and neural networks. By the end, you'll be able to implement a neural network that learns a policy to solve classic control tasks. Here is what this journey will involve:\n",
        "\n",
        "- **Exploring Policy Gradients**: We'll introduce you to the core ideas behind policy gradient methods and why they're a natural fit for problems where traditional RL methods fall short.\n",
        "- **Neural Network as Policy Approximators**: Learn how deep neural networks can be used to approximate the policy, enabling agents to navigate complex environments.\n",
        "- **Implementing the REINFORCE (and more) Algorithm**: You'll code the REINFORCE algorithm, one of the most widely used policy gradient methods.\n",
        "- **Training an Agent in a Classic Control Environment**: Finally, we'll put everything together and train an agent to master a classic control environment using your own implementation.\n",
        "\n",
        "The exercises and explanations provided here are designed to merge theoretical understanding with practical coding. By the time you finish, you'll have hands-on experience with policy gradient methods and their applications in reinforcement learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AIY_NiILj9iV",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e976b62-fe55-46d5-a709-2daf9c346964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "#@title Install dependecies\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CtpBWmRzj9iW",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78cb9625-b498-4f8b-f8ee-0305702bf4f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "np.bool8 = np.bool\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from matplotlib import rc\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import gym\n",
        "\n",
        "# making plots pretty\n",
        "sns.set_palette(\"deep\")\n",
        "rc('animation', html='jshtml')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# for reproducibility's sake!\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "print(gym.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZBNOJpymj9iX"
      },
      "outputs": [],
      "source": [
        "#@title Some helper functions\n",
        "\n",
        "def evaluate(env, policy, gamma=1., num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param env: (Env object) the Gym environment\n",
        "    :param policy: (BasePolicy object) the policy in stable_baselines3\n",
        "    :param gamma: (float) the discount factor\n",
        "    :param num_episodes: (int) number of episodes to evaluate it\n",
        "    :return: (float) Mean reward for the last num_episodes\n",
        "    \"\"\"\n",
        "    all_episode_rewards = []\n",
        "    for i in range(num_episodes): # iterate over the episodes\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        discounter = 1.\n",
        "        obs = env.reset()\n",
        "        frames = []\n",
        "        while not done: # iterate over the steps until termination\n",
        "            action = policy.draw_action(obs)\n",
        "            obs, reward, terminated, truncated = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_rewards.append(reward * discounter) # compute discounted reward\n",
        "            discounter *= gamma\n",
        "            frames.append(env.render(mode=\"rgb_array\"))\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    std_episode_reward = np.std(all_episode_rewards) / np.sqrt(num_episodes - 1)\n",
        "    print(\"Mean reward:\", mean_episode_reward,\n",
        "          \"Std reward:\", std_episode_reward,\n",
        "          \"Num episodes:\", num_episodes)\n",
        "\n",
        "    return mean_episode_reward, std_episode_reward, frames\n",
        "\n",
        "\n",
        "\n",
        "def plot_results(results):\n",
        "    \"\"\"\n",
        "    Plot the results of the experiments\n",
        "    :param results: (list of tuples) each tuple contains the mean and std of the return\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    _mean = []\n",
        "    _std = []\n",
        "    for m, s, _ in results:\n",
        "        _mean.append(m)\n",
        "        _std.append(s)\n",
        "\n",
        "    _mean = np.array(_mean)\n",
        "    _std = np.array(_std)\n",
        "\n",
        "    ts = np.arange(len(_mean))\n",
        "    plt.plot(ts, _mean)\n",
        "    plt.fill_between(ts, _mean-_std, _mean+_std, alpha=.2)\n",
        "\n",
        "    plt.xlabel('Trajectories')\n",
        "    plt.ylabel('Average return')\n",
        "    plt.legend(loc='lower right')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def collect_rollouts(env, policy, m, T):\n",
        "    \"\"\"\n",
        "    Collects m rollouts by running the policy in the\n",
        "        environment\n",
        "    :param env: (Env object) the Gym environment\n",
        "    :param policy: (Policy object) the policy\n",
        "    :param gamma: (float) the discount factor\n",
        "    :param m: (int) number of episodes per iterations\n",
        "    :param K: (int) maximum number of iterations\n",
        "    :param theta0: (ndarray) initial parameters (d,)\n",
        "    :param alpha: (float) the constant learning rate\n",
        "    :param T: (int) the trajectory horizon\n",
        "    :return: (list of lists) one list per episode\n",
        "                each containing triples (s, a, r)\n",
        "    \"\"\"\n",
        "\n",
        "    ll = []\n",
        "    for j in range(m):\n",
        "        s = env.reset()\n",
        "        t = 0\n",
        "        done = False\n",
        "        l = []\n",
        "        while t < T and not done:\n",
        "            a = policy.draw_action(s)\n",
        "            s1, r, done, _ = env.step(a)\n",
        "            l.append((s, a, r))\n",
        "            s = s1\n",
        "            t += 1\n",
        "        ll.append(l)\n",
        "    return ll\n",
        "\n",
        "def animate(data, interval=200):\n",
        "  fig = plt.figure(1)\n",
        "  img = plt.imshow(data[0][0])\n",
        "  plt.axis('off')\n",
        "\n",
        "  def animate(i):\n",
        "    img.set_data(data[i][0])\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, frames=len(data), interval=interval)\n",
        "  plt.close(1)\n",
        "  return anim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recap from first part\n",
        "\n",
        "- An **environment** represent the task or the problem that we are trying to solve. Our agent directly interacts with the environment through an **action** collecting a **reward**, which will drive the learning process, and observing the variation of the **state**.\n",
        "- An **episode** (also called **trajectory** or **rollout**) refers to the sequence of states, actions and rewards an agent experiences as it interacts with the environment over time."
      ],
      "metadata": {
        "id": "_JelZ8AmV9CM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ixha06AJj9iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d2cd2c-464b-45ac-eb70-fd1c6cc572dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space \n",
            "\n",
            "The State Space size is:  4\n",
            "Sample observation [ 4.2716417e+00  1.8906295e+38  2.4119624e-01 -1.5576345e+38]\n",
            "Action space \n",
            "\n",
            "The Action Space size is:  2\n",
            "Action Space Sample 0\n"
          ]
        }
      ],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "# Create the env\n",
        "env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "\n",
        "# Create the evaluation env\n",
        "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "\n",
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n\n",
        "\n",
        "print(\"Observation space \\n\")\n",
        "print(\"The State Space size is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n",
        "\n",
        "print(\"Action space \\n\")\n",
        "print(\"The Action Space size is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocTjSulNj9iY"
      },
      "source": [
        "# Introduction to Policy Gradient RL\n",
        "\n",
        "## Policies\n",
        "\n",
        "In reinforcement learning (RL), a **policy** defines the behavior of an agent by mapping states to actions. It determines how the agent makes decisions at each time step while interacting with the environment. Formally, a policy $\\pi$ is a function that takes a state as inpunt and returns an action.\n",
        "We distinguish two types of policies:\n",
        "- **Deterministic Policy**: Always returns the same action for a given state.\n",
        "\n",
        "$$\n",
        "  a = \\pi(s)\n",
        "$$\n",
        "\n",
        "- **Stochastic Policy**: Returns a probability distribution over possible actions. The agent samples an action from this distribution.\n",
        "\n",
        "$$\n",
        "  \\pi(a | s) = P(a | s)\n",
        "$$\n",
        "\n",
        "In stochastic policies, $\\pi(a∣s)$ represents the probability of taking action $a$ given the state $s$.\n",
        "\n",
        "A stochastic policy can be directly parametrised by a function with paramethers $\\theta$ mapping states to action probabilities: so that the actions of the agent in each state are sampled from some distribution $\\pi_\\theta(a|s)$. For instance such function may be parametrised by a neural network with parameters $\\theta$.\n",
        "\n",
        "## Policy Gradient methods\n",
        "\n",
        "Policy gradient methods aim to learn the stochastic policy mapping states to action probabilities by interacting with the environment and updating the parameters of the policy based on the observed rewards. To do so we need a loss function or *objective*. In RL, the general objective is to maximise the expected episode return when taking actions in the environment according to the policy.  So, the task of a policy gradient method will be to update the neural network parameters $\\theta$ so as to maximise:\n",
        "\n",
        "$$J(\\pi_\\theta)=\\mathrm{E}_{\\tau\\sim\\pi_\\theta}\\ [R(\\tau)],$$\n",
        "\n",
        "where $\\mathrm{E}$ denotes that we take the *expectation* over the states that we could be in, and the actions that we could pick in those states, $\\tau$ is again shorthand for episode, and $R(\\tau)$ denotes the return of episode $\\tau$. One way to then find the parameters $\\theta$ that maximise such objective $J(\\pi_\\theta)$  is to do so iteratively by performing gradient ascent on $J(\\pi_\\theta)$ with respect to the parameters $\\theta$.\n",
        "\n",
        "$$\\theta_{k+1}=\\theta_k + \\alpha \\nabla J(\\pi_\\theta)|_{\\theta_{k}},$$\n",
        "\n",
        "where $\\nabla J(\\pi_\\theta)|_{\\theta_{k}}$ is the gradient of the expected return with respect to the policy parameters $\\theta_k$ and $\\alpha$ is the step size. This quantity, $\\nabla J(\\pi_\\theta)$, is also called the **policy gradient**. If we can compute the policy gradient, then we will have a means by which to directly optimise our policy. Luckily, as it turns out, there is a [way](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html) for us to compute the policy gradient as:\n",
        "\n",
        "\n",
        "$$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) R(\\tau)].$$\n",
        "\n",
        "Where $\\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t})$ is called the *score*.\n",
        "\n",
        "\n",
        "## Softmax Policies\n",
        "\n",
        "In particular, we will consider **softmax policy**. A softmax policy is a type of stochastic policy where the neural network predicts for each action an un-normalised *preference* $h_\\theta(s,a)$. The probability of each action $a$ is then calculated as:\n",
        "\n",
        "$$\n",
        "  \\pi(a|s) = \\frac{\\exp(h_\\theta(s,a))}{\\sum_{a^\\prime}\\exp(h_\\theta(s,a^\\prime))}\n",
        "$$\n",
        "\n",
        "For such class of policies, we can then compute the *score* of a softmax policy as:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}(a \\mid s) & =\\nabla_{\\boldsymbol{\\theta}} h_{\\boldsymbol{\\theta}}(s, a)-\\nabla_{\\boldsymbol{\\theta}} \\log \\sum_{a^{\\prime} \\in A} \\exp \\left(h_{\\boldsymbol{\\theta}}\\left(s, a^{\\prime}\\right) \\right) \\\\\n",
        "& =\\nabla_{\\boldsymbol{\\theta}} h_{\\boldsymbol{\\theta}}(s, a)-\\frac{\\sum_{a^{\\prime} \\in A} \\nabla_{\\boldsymbol{\\theta}} \\exp \\left(h_{\\boldsymbol{\\theta}}\\left(s, a^{\\prime}\\right)\\right)}{\\sum_{a^{\\prime} \\in A} \\exp \\left(h_{\\boldsymbol{\\theta}}\\left(s, a^{\\prime}\\right)\\right)} \\\\\n",
        "& =\\nabla_{\\boldsymbol{\\theta}} h_{\\boldsymbol{\\theta}}(s, a)-\\sum_{a^{\\prime} \\in A} \\frac{\\exp \\left(h_{\\boldsymbol{\\theta}}\\left(s, a^{\\prime}\\right)\\right)}{\\sum_{a^{\\prime \\prime} \\in A} \\exp \\left(h_{\\boldsymbol{\\theta}}\\left(s, a^{\\prime \\prime}\\right)\\right)} \\nabla_{\\boldsymbol{\\theta}} h_{\\boldsymbol{\\theta}}\\left(s, a^{\\prime}\\right) \\\\\n",
        "& =\\left(\\nabla_{\\boldsymbol{\\theta}} h_{\\boldsymbol{\\theta}}(s, a)-\\underset{a^{\\prime} \\sim \\pi_{\\boldsymbol{\\theta}}(\\cdot \\mid s)}{\\mathbb{E}}\\left[\\nabla_{\\boldsymbol{\\theta}} h_{\\boldsymbol{\\theta}}\\left(s, a^{\\prime}\\right)\\right]\\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "In the next cell we will implement a policy network with a softmax output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdnJMHw6j9ia"
      },
      "source": [
        "## ⭐ Exercise\n",
        "\n",
        "1) **Policy neural network.**\n",
        "Our policy neural network policy will take the observation as input and passes it through an MLP with `len(num_hiddens)` hidden layers and then outputs one scalar value for each of the possible actions (`2` in CartPole). The outputs of our policy network are the action probabilities provided by a softmax function. Have a look [here](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) to understand how to use a softmax using PyTorch. We will use PyTorch for the neural network training. Please have a look [here](https://pytorch.org/docs/stable/index.html) to understand how PyTorch implements neural network layers and activation functions.\n",
        "\n",
        "2) **Actor function.**\n",
        "Next we implement `draw_action` function, which takes network parameters, timestep and random key and returns an action of the policy. Fill in the gaps in the `actor_step` function. Use [`torch.distributions.categorical`](https://pytorch.org/docs/stable/distributions.html#categorical) function to sample an action given the logits.\n",
        "\n",
        "3) **Compute the gradient log.**\n",
        "Finally, we implement the function that computes the score function (or gradient log) of the parameters, which we will later use to estimate the policy gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u0aHiq6Lj9ia"
      },
      "outputs": [],
      "source": [
        "class SoftmaxPolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=16):\n",
        "        super(SoftmaxPolicyNetwork, self).__init__()\n",
        "        # -----------------------------------#\n",
        "        # Define the neural network layers\n",
        "        self.fc1 = nn.Linear(input_dim,hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        # -----------------------------------#\n",
        "        self.dim = self.get_parameters().size(dim=0)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # -----------------------------------#\n",
        "        # Forward pass through the network\n",
        "        x = self.fc1(state)\n",
        "        logits = self.fc2(x)\n",
        "        action_probs = nn.Softmax(dim=-1)(logits)\n",
        "        return action_probs\n",
        "        # -----------------------------------#\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return nn.utils.parameters_to_vector(self.parameters()).detach()\n",
        "\n",
        "    def set_parameters(self, new_params):\n",
        "        nn.utils.vector_to_parameters(new_params, self.parameters())\n",
        "\n",
        "    def draw_action(self, state):\n",
        "        # -----------------------------------#\n",
        "        # Select an action based on the state using the softmax policy.\n",
        "        action_probs = self.forward(state)\n",
        "        action = Categorical(action_probs).sample()\n",
        "        return action\n",
        "        # -----------------------------------#\n",
        "\n",
        "    def grad_log(self, state, action):\n",
        "        \"\"\"\n",
        "        Compute the score function (policy gradient) for a given state and action.\n",
        "\n",
        "        Args:\n",
        "            state: A tensor representing the current state (1D).\n",
        "            action: The action taken (int).\n",
        "\n",
        "        Returns:\n",
        "            score: A 1D tensor representing the gradient of the log-probability\n",
        "                   of the action with respect to the network parameters.\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(state)\n",
        "        action_probs = self.forward(state)\n",
        "\n",
        "        # -----------------------------------#\n",
        "        # One-hot encoding of the selected action\n",
        "        action_one_hot = one_hot(torch.tensor(action), num_classes=action_probs.size(-1))\n",
        "\n",
        "        # Compute the gradient of the log-probability w.r.t logits\n",
        "        score = action_one_hot - action_probs\n",
        "        # -----------------------------------#\n",
        "\n",
        "        # Compute the gradient of the network parameters using chain rule\n",
        "        # First, we propagate the score through the second layer (fc2)\n",
        "        fc2_grad_wrt_logits = score.view(-1, 1)  # score is d(log P(a|s)) / d(logits)\n",
        "        fc2_grad_wrt_weights = torch.matmul(fc2_grad_wrt_logits, F.relu(self.fc1(state)).view(1, -1))  # Chain rule for fc2 weights\n",
        "        fc2_grad_wrt_bias = fc2_grad_wrt_logits.view(-1)  # Chain rule for fc2 bias\n",
        "\n",
        "        # Now, we compute the gradient w.r.t. the output of the first layer\n",
        "        d_fc1_output = torch.matmul(self.fc2.weight.T, score) * (self.fc1(state) > 0).float()  # Backprop through ReLU\n",
        "        fc1_grad_wrt_weights = torch.matmul(d_fc1_output.view(-1, 1), state.view(1, -1))  # Chain rule for fc1 weights\n",
        "        fc1_grad_wrt_bias = d_fc1_output.view(-1)  # Chain rule for fc1 bias\n",
        "\n",
        "        # Concatenate all gradients into a single vector\n",
        "        score_vector = torch.cat([fc1_grad_wrt_weights.view(-1), fc1_grad_wrt_bias.view(-1),\n",
        "                                  fc2_grad_wrt_weights.view(-1), fc2_grad_wrt_bias.view(-1)])\n",
        "\n",
        "        return score_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJWRn0MDj9ib"
      },
      "source": [
        "# Policy gradient algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdUE0mR5j9ib"
      },
      "source": [
        "## REINFORCE\n",
        "\n",
        "REINFORCE is a simple RL algorithm that uses the policy gradient to find the optimal policy by sampling $\\nabla_\\theta J(\\theta)$ and for each set of states and actions considered in each update just increases the probability of choosing the actions (*reinforcing* actions) that were taken proportionally to the cumulative reward observed after taking them.\n",
        "\n",
        "In REINFORCE we just take the general expression of our gradient of the expected return\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_\\pi\\left[R(\\tau) \\nabla_\\theta \\log \\pi_\\theta\\left(A_t \\mid S_t\\right)\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "and just replaces the expectation over all possible trajectories that the agent could traverse in the environment with the *monte-carlo* sampling of a single trajectory:\n",
        "\n",
        "$$\n",
        "\\begin{aligned} \\widehat{\\nabla}_{\\theta}^{\\text{REINFORCE}} J(\\boldsymbol{\\theta})\n",
        "& = \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\log \\pi_\\theta(a_{i} | s_{i})G_i\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where $G_i$ represents the **return** (also known as **rewards-to-go**) from step i onwards.\n",
        "\n",
        "The process is pretty straightforward:\n",
        "\n",
        "- Initialize the policy parameter $\\theta$.\n",
        "- Generate one trajectory on policy $\\pi_\\theta$: .\n",
        "- For t=1, 2, … , T:\n",
        "  - Estimate the the return $R$;\n",
        "  - Update policy parameters: $\\theta \\leftarrow \\theta+\\alpha \\widehat{\\nabla}_{\\theta}^{\\text{REINFORCE}} J(\\theta)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBVpg2lHuRuE"
      },
      "source": [
        "### ⭐ Exercise\n",
        "\n",
        "In the next cell implement the REINFORCE estimator given the formal definition presented in the previous section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97Re7vFAj9ib"
      },
      "outputs": [],
      "source": [
        "def reinforce(rollouts, policy, gamma):\n",
        "\n",
        "    grad = 0\n",
        "\n",
        "    for roll in rollouts:\n",
        "        H = len(roll)\n",
        "        disc_rew = torch.zeros((H, 1))\n",
        "        scores = torch.zeros((H, policy.dim))\n",
        "\n",
        "        # -----------------------------------#\n",
        "        # Retrive the score of the policy and compute the gradient\n",
        "        for t in range(H):\n",
        "            s, a, r = roll[t]\n",
        "            # We use gamma as the discount factor to account for the time it took to get a particular reward.\n",
        "            # E.G. achieving reward +1 far in the future should be less desirable than achieving reward +1 now.\n",
        "            disc_rew[t] = gamma ** t * r\n",
        "            scores[t] = policy.grad_log(s, a)\n",
        "\n",
        "        cum_scores = torch.sum(scores,dim=-1)\n",
        "        grad += TODO\n",
        "        # -----------------------------------#\n",
        "\n",
        "    return grad / len(rollouts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V73r7qLyj9ic"
      },
      "source": [
        "# Training loop\n",
        "\n",
        "Finally, let's run the learning loop.\n",
        "As the structure of the agent is very similar to Q-learning agent, the learning loop is very similar as well. Notice that we perform a learning step on a batch of samples instead of a single datapoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvGXTJEAj9ic"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(env, policy, gamma, m, K, alpha, T):\n",
        "    \"\"\"\n",
        "    Train a policy\n",
        "    :param env: (Env object) the Gym environment\n",
        "    :param policy: (Policy object) the policy\n",
        "    :param gamma: (float) the discount factor\n",
        "    :param m: (int) number of episodes per iterations\n",
        "    :param K: (int) maximum number of iterations\n",
        "    :param alpha: (float) the constant learning rate\n",
        "    :param T: (int) the trajectory horizon\n",
        "    :return: list (ndarray, ndarray) the evaluations\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Evaluate the initial policy\n",
        "    res = evaluate(env, policy, gamma)\n",
        "    results.append(res)\n",
        "\n",
        "    for k in range(K):\n",
        "\n",
        "        print('Iteration:', k)\n",
        "\n",
        "        # Generate rollouts\n",
        "        rollouts = collect_rollouts(env, policy, m, T)\n",
        "\n",
        "        # Get policy parameter\n",
        "        theta = policy.get_parameters()\n",
        "\n",
        "        # Call your Gradient estimator\n",
        "        pg = reinforce(rollouts, policy, gamma)\n",
        "\n",
        "        # Update policy parameter\n",
        "        theta = theta + alpha * pg\n",
        "\n",
        "        # Set policy parameters\n",
        "        policy.set_parameters(theta)\n",
        "\n",
        "        # Evaluate the updated policy\n",
        "        res = evaluate(env, policy, gamma)\n",
        "        results.append(res)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_NaLLYQuogK"
      },
      "source": [
        "## ⭐ Exercise\n",
        "\n",
        "- Choose the parameters (number of training episodes, numbers of hidden units in MLP, learning rate, batch size) so that the average return of the agent increases with training and ends up being greater than $100$.\n",
        "- Run the learning loop and visualise the epsiode returns. Look at the animation of the last episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjyhmnHXj9ic"
      },
      "outputs": [],
      "source": [
        "discount_factor = 0.999 # @param\n",
        "batch_size = 100\n",
        "iterations = 30 # @param\n",
        "learning_rate = 0.001 # @param\n",
        "trajectory_length = 200 # @param\n",
        "n_hidden = 8 # @param\n",
        "\n",
        "# Instantiate the policy\n",
        "policy = SoftmaxPolicyNetwork(s_size, a_size, n_hidden)\n",
        "\n",
        "# Start training\n",
        "results = train(env, policy, discount_factor, batch_size, iterations, learning_rate, trajectory_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMnroYHij9id"
      },
      "outputs": [],
      "source": [
        "# Plot the results of the training\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R3q6s_NFR-y"
      },
      "outputs": [],
      "source": [
        "# Evaluate the policy and animate the results\n",
        "perf_mean, perf_std, frames = evaluate(eval_env, policy, num_episodes=1)\n",
        "\n",
        "# Animate the last episode\n",
        "animate(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extensions\n",
        "\n",
        "There is a large literature improving the previously described algorithm along different dimensions\n",
        "\n",
        "### Variance reduction:\n",
        "\n",
        "Policy gradient updates can be high variance because they depend on the sampled Monte Carlo return. To reduce variance it's usually a good idea to subtract a baseline\n",
        "\n",
        "$$\n",
        "\\begin{aligned} \\widehat{\\nabla}_{\\theta}^{\\text{REINFORCE}} J(\\boldsymbol{\\theta})\n",
        "& = \\frac{1}{N} \\sum_{i=1}^N (G_i - b_i(s_i)) \\nabla_\\theta \\log \\pi_\\theta(a_{i} | s_{i})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Interestingly, this is a valid update for any choice of $b_i(s)$ as long as it depends only on the state and not the action. And for suitable choices of $b_i(s)$ it can reduce the variance. A common choice is to use the value of the state s as baseline. This can be estimated parametrically (e.g. by a learning a value function as in the previous colab). Alternatively, it can be estimated non parametrically (by sampling many trajectories starting in s and using the average return across them as baseline).\n",
        "\n",
        "The latter approach is what is used in the popular [GRPO](https://arxiv.org/pdf/2402.03300) algorithm.\n",
        "\n",
        "### Off-policy learning\n",
        "\n",
        "In their most naive form policy gradient updates require the trajectories to be sampled from the current policy for the gradient to be an unbiased ascent direction of the RL objective. However, for various reason we often have data sampled from a different (e.g. slightly older) policy. In this cases we can modify the naive update by adding importance sampling corrections (see for instance PPO for the most popular instantiation of this concept)."
      ],
      "metadata": {
        "id": "PHOM2dKqLKEP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGWTD_OlFciV"
      },
      "source": [
        "# 🥳 Congratulations on completing the second part of this tutorial, great job!!!\n",
        "\n",
        "In this part you learnt how policy gradient works and how to combine it with neural networks. Next, we will look into RL for LLMs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "phd-env-39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}